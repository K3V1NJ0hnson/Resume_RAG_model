{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kevin\\Projects\\Resume_RAG_Model\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import fitz\n",
    "from bs4 import BeautifulSoup\n",
    "from semantic_text_splitter import TextSplitter\n",
    "import sys, numpy as np\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2125b4b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\kevin\\onedrive\\resume_rag_model\\.venv\\lib\\site-packages (2.20.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\kevin\\onedrive\\resume_rag_model\\.venv\\lib\\site-packages (from openai) (4.12.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\kevin\\onedrive\\resume_rag_model\\.venv\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\kevin\\onedrive\\resume_rag_model\\.venv\\lib\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in c:\\users\\kevin\\onedrive\\resume_rag_model\\.venv\\lib\\site-packages (from openai) (0.13.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\kevin\\onedrive\\resume_rag_model\\.venv\\lib\\site-packages (from openai) (2.12.5)\n",
      "Requirement already satisfied: sniffio in c:\\users\\kevin\\onedrive\\resume_rag_model\\.venv\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\kevin\\onedrive\\resume_rag_model\\.venv\\lib\\site-packages (from openai) (4.67.3)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\kevin\\onedrive\\resume_rag_model\\.venv\\lib\\site-packages (from openai) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\kevin\\onedrive\\resume_rag_model\\.venv\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
      "Requirement already satisfied: certifi in c:\\users\\kevin\\onedrive\\resume_rag_model\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\kevin\\onedrive\\resume_rag_model\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\kevin\\onedrive\\resume_rag_model\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\kevin\\onedrive\\resume_rag_model\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\kevin\\onedrive\\resume_rag_model\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\kevin\\onedrive\\resume_rag_model\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\kevin\\onedrive\\resume_rag_model\\.venv\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.3 -> 26.0.1\n",
      "[notice] To update, run: C:\\Users\\Kevin\\OneDrive\\Resume_RAG_Model\\.venv\\Scripts\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53ba4c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Using cached openai-2.20.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (1.2.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from openai) (4.12.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from openai) (0.28.1)\n",
      "Collecting jiter<1,>=0.10.0 (from openai)\n",
      "  Using cached jiter-0.13.0-cp313-cp313-win_amd64.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from openai) (2.12.5)\n",
      "Collecting sniffio (from openai)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from openai) (4.67.3)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from openai) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
      "Requirement already satisfied: certifi in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Using cached openai-2.20.0-py3-none-any.whl (1.1 MB)\n",
      "Using cached jiter-0.13.0-cp313-cp313-win_amd64.whl (202 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Installing collected packages: sniffio, jiter, openai\n",
      "\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   ---------------------------------------- 3/3 [openai]\n",
      "\n",
      "Successfully installed jiter-0.13.0 openai-2.20.0 sniffio-1.3.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.3 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install openai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fa68a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe621b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (4.14.3)\n",
      "Requirement already satisfied: pymupdf in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (1.27.1)\n",
      "Requirement already satisfied: semantic-text-splitter in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (0.29.0)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (5.2.2)\n",
      "Requirement already satisfied: chromadb in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (1.5.0)\n",
      "Requirement already satisfied: soupsieve>=1.6.1 in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from beautifulsoup4) (2.8.3)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from beautifulsoup4) (4.15.0)\n",
      "Requirement already satisfied: transformers<6.0.0,>=4.41.0 in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from sentence-transformers) (5.1.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from sentence-transformers) (1.4.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from sentence-transformers) (2.10.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from sentence-transformers) (2.4.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from sentence-transformers) (1.8.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from sentence-transformers) (1.17.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from sentence-transformers) (4.67.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (26.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (2026.1.15)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.22.2)\n",
      "Requirement already satisfied: typer-slim in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.23.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.20.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2026.2.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (0.28.1)\n",
      "Requirement already satisfied: shellingham in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.5.4)\n",
      "Requirement already satisfied: anyio in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (4.12.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (0.16.0)\n",
      "Requirement already satisfied: build>=1.0.3 in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from chromadb) (1.4.0)\n",
      "Requirement already satisfied: pydantic>=1.9 in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from chromadb) (2.12.5)\n",
      "Requirement already satisfied: pybase64>=1.4.1 in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from chromadb) (1.4.3)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.40.0)\n",
      "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from chromadb) (5.4.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from chromadb) (1.24.1)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from chromadb) (1.39.1)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from chromadb) (1.39.1)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from chromadb) (1.39.1)\n",
      "Requirement already satisfied: pypika>=0.48.9 in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from chromadb) (0.51.1)\n",
      "Requirement already satisfied: overrides>=7.3.1 in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from chromadb) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from chromadb) (1.78.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from chromadb) (5.0.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from chromadb) (0.23.0)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from chromadb) (35.0.0)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from chromadb) (9.1.4)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from chromadb) (5.2.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from chromadb) (3.11.7)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from chromadb) (14.3.2)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from chromadb) (4.26.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.7 in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.32.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.2 in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: backoff>=1.10.0 in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: distro>=1.5.0 in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from requests<3.0,>=2.7->posthog<6.0.0,>=2.4.0->chromadb) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from requests<3.0,>=2.7->posthog<6.0.0,>=2.4.0->chromadb) (2.6.3)\n",
      "Requirement already satisfied: pyproject_hooks in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from build>=1.0.3->chromadb) (0.4.6)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (25.4.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.25.0 in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (0.30.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.9.0)\n",
      "Requirement already satisfied: requests-oauthlib in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: durationpy>=0.7 in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (25.12.19)\n",
      "Requirement already satisfied: protobuf in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (6.33.5)\n",
      "Requirement already satisfied: sympy in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (1.14.0)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.1)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.57 in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.72.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.39.1 in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.39.1)\n",
      "Requirement already satisfied: opentelemetry-proto==1.39.1 in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.39.1)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.60b1 in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.60b1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from pydantic>=1.9->chromadb) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from pydantic>=1.9->chromadb) (0.4.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from rich>=10.11.0->chromadb) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (82.0.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from typer>=0.9.0->chromadb) (8.3.1)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from typer>=0.9.0->chromadb) (0.0.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.7.1)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.2.1)\n",
      "Requirement already satisfied: watchfiles>=0.13 in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.1)\n",
      "Requirement already satisfied: websockets>=10.4 in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from requests-oauthlib->kubernetes>=28.1.0->chromadb) (3.3.1)\n",
      "Requirement already satisfied: joblib>=1.3.0 in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.3 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install beautifulsoup4 pymupdf semantic-text-splitter sentence-transformers chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5a7ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\kevin\\onedrive\\resume_rag_model\\.venv\\lib\\site-packages (4.14.3)\n",
      "Requirement already satisfied: soupsieve>=1.6.1 in c:\\users\\kevin\\onedrive\\resume_rag_model\\.venv\\lib\\site-packages (from beautifulsoup4) (2.8.3)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\kevin\\onedrive\\resume_rag_model\\.venv\\lib\\site-packages (from beautifulsoup4) (4.15.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.3 -> 26.0.1\n",
      "[notice] To update, run: C:\\Users\\Kevin\\OneDrive\\Resume_RAG_Model\\.venv\\Scripts\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install beautifulsoup4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58925aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyMuPDF in c:\\users\\kevin\\projects\\resume_rag_model\\.venv\\lib\\site-packages (1.27.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.3 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f62688f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load PDFs\n",
    "def load_pdf(file_path):\n",
    "    doc = fitz.open(file_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9236347e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load HTML\n",
    "def load_html(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        soup = BeautifulSoup(f, 'html.parser')\n",
    "    return soup.get_text(separator='\\n', strip=True) #clean text extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfe1918",
   "metadata": {},
   "source": [
    "## Documents & Webpages\n",
    "\n",
    "Here we will load in PDFs and webpages for our model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "62508852",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Kevin_resume = load_pdf('Kevin_Johnson_res.pdf')\n",
    "Scratch_CNN = load_html('K3V1NJ0hnson_Scratch_Conv_Neural_Network.html')\n",
    "Hurricane_project = load_html('K3V1NJ0hnson_Hurricane_Season_Predictors.html')\n",
    "Capstone_project = load_html('K3V1NJ0hnson_Rental_Growth_Pre_Post_COVID.html')\n",
    "Home_Loan_Approval_predictor = load_html(\"K3V1NJ0hnson_Home-Loan-Approval-ML-Project.html\")\n",
    "Customer_potential = load_html(\"K3V1NJ0hnson_Customer_Potential.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad34dccc",
   "metadata": {},
   "source": [
    "## Clean the text \n",
    "\n",
    "Next we will clean the text for better utilization of our RAG model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c0e7b391",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text) # Normalize whitespace\n",
    "    text = re.sub(r'Page \\d+', '', text) #REmove page numbers\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "f7f4830e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 120\n",
      "{'category': 'Resume', 'source': 'Kevin_Johnson_res.pdf', 'chunk_index': 0}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "chunk_size = 300 # tweak 800/1000/etc.\n",
    "splitter = TextSplitter(chunk_size)\n",
    "\n",
    "docs = [\n",
    "    (\"Resume\", \"Kevin_Johnson_res.pdf\", clean_text(Kevin_resume)),\n",
    "    (\"GitHub\", \"K3V1NJ0hnson_Scratch_Conv_Neural_Network.html\", clean_text(Scratch_CNN)),\n",
    "    (\"GitHub\", \"K3V1NJ0hnson_Hurricane_Season_Predictors.html\", clean_text(Hurricane_project)),\n",
    "    (\"GitHub\", \"K3V1NJ0hnson_Rental_Growth_Pre_Post_COVID.html\", clean_text(Capstone_project)),\n",
    "    (\"GitHub\", \"K3V1NJ0hnson_Home-Loan-Approval-ML-Project.html\", clean_text(Home_Loan_Approval_predictor)),\n",
    "    (\"GitHub\", \"K3V1NJ0hnson_Customer_Potential.html\", clean_text(Customer_potential))\n",
    "]\n",
    "\n",
    "all_chunks = []\n",
    "for category, source_name, text in docs:\n",
    "    chunks = splitter.chunks(text)\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        all_chunks.append(\n",
    "            {\n",
    "                \"text\": chunk,\n",
    "                \"metadata\":{\n",
    "                    \"category\": category,\n",
    "                    \"source\": source_name,\n",
    "                    \"chunk_index\": i\n",
    "                }\n",
    "            })\n",
    "        \n",
    "print(f\"Total chunks: {len(all_chunks)}\")\n",
    "print(all_chunks[0][\"metadata\"])  # Print first chunk as sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "a9ca45a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:00<00:00, 1391.18it/s, Materializing param=pooler.dense.weight]                             \n",
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model loaded.\n"
     ]
    }
   ],
   "source": [
    "# embedding model from Hugging Face.\n",
    "# May change this later to something more robust\n",
    "embed_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "print(\"Embedding model loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "b66fb873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 120\n",
      "Example metadata: {'category': 'Resume', 'source': 'Kevin_Johnson_res.pdf', 'chunk_index': 0}\n"
     ]
    }
   ],
   "source": [
    "texts = [c[\"text\"] for c in all_chunks]\n",
    "metadatas = [c[\"metadata\"] for c in all_chunks]\n",
    "\n",
    "print(f\"Number of chunks: {len(texts)}\")\n",
    "print(\"Example metadata:\", metadatas[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "0022d27f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00,  9.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (120, 384)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "embeddings = embed_model.encode(\n",
    "    texts, \n",
    "    batch_size=32,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True,\n",
    ")\n",
    "\n",
    "print(\"Embeddings shape:\", embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "3b14ea48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "[-0.04944386 -0.03049594  0.09772719  0.01620435 -0.02522486  0.01099211\n",
      " -0.01570248 -0.00734629  0.01563937 -0.02282   ]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(type(embeddings))       # should be <class 'numpy.ndarray'>\n",
    "print(embeddings[0][:10])     # first 10 dims of first chunk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "d9d40bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chroma persistent client ready at: c:\\Users\\Kevin\\Projects\\Resume_RAG_Model\\resume_chroma_db\n"
     ]
    }
   ],
   "source": [
    "# Initialize Chroma client\n",
    "current_dir = os.getcwd()\n",
    "db_path = os.path.join(current_dir, \"resume_chroma_db\")\n",
    "\n",
    "client = chromadb.PersistentClient(\n",
    "    path=db_path,\n",
    "    settings=Settings(anonymized_telemetry=False)\n",
    ")\n",
    "\n",
    "collection = client.get_or_create_collection(\"Kevin_profile\")\n",
    "print(f\"Chroma persistent client ready at: {db_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "b766acde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ About Me section successfully added to the Vector Database!\n"
     ]
    }
   ],
   "source": [
    "# 1.\"About Me\" text\n",
    "about_me_text = \"\"\"\n",
    "### About Kevin: The Decathlete of Data\n",
    "\n",
    "### EDUCATION AND DEGREES\n",
    "- **Bachelor of Science in Data Science** (Arizona State University)\n",
    "- **Bachelor of Science in Supply Chain Management** (University of Houston)\n",
    "\n",
    "I am a dual-degree Data Scientist and former D1 scholarship athlete for the University of Houston. \n",
    "I compete in the Decathlon, with my favorite events being the 110m hurdles, discus, and high jump. \n",
    "\n",
    "### Key Achievements & Background\n",
    "- **Elite Versatility:** I was an All-Conference athlete my freshman year and served as Team Captain. \n",
    "- **The Competitive Edge:** I once defeated an Olympic gold medalist in his respective event during a college meet.\n",
    "- **Dual Degrees:** I hold a B.S. in Supply Chain Management (Univ. of Houston) and a B.S. in Data Science (Arizona State Univ.).\n",
    "- **Unmatched Discipline:** Outside of tech, I've completed 10,000+ rideshare trips with a perfect 5.0-star rating.\n",
    "\n",
    "### Professional Philosophy\n",
    "I am a perfectionist who thrives on mentally stimulating challenges. I find my 'flow' when diving deep \n",
    "into complex AI and Data Science problems. While I hold my work to an incredibly high standard, \n",
    "I am passionate about building production-ready AI tools that solve real-world problems.\n",
    "\n",
    "### Geography\n",
    "Raised in Houston, TX; spent 3 years in Las Vegas and 3 years in Orange County, CA, before returning to Texas.\n",
    "\"\"\"\n",
    "\n",
    "# 2. Add it to collection\n",
    "collection.add(\n",
    "    ids=[\"about_me_01\"],\n",
    "    documents=[about_me_text],\n",
    "    metadatas=[{\"category\": \"About Me\", \"source\": \"Personal Bio\"}]\n",
    ")\n",
    "\n",
    "print(\"‚úÖ About Me section successfully added to the Vector Database!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "30d20244",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_context = \"\"\"\n",
    "### PROJECT TECH SPECS: Resume RAG Assistant\n",
    "This specific project is built using a RAG (Retrieval-Augmented Generation) architecture.\n",
    "Kevin built this project to help recruiters understand his background and why you should give him an interview\n",
    "- **Vector DB:** ChromaDB\n",
    "- **Embedding Model:** sentence-transformers/all-MiniLM-L6-v2\n",
    "- **LLM:** OpenAI GPT-4o-mini\n",
    "- **Data Sources:** PDF Resumes, GitHub Markdown files, and manual Bio strings.\n",
    "- **Features:** Custom query normalization to handle typos and semantic search to find relevant experience across multiple documents.\n",
    "\"\"\"\n",
    "\n",
    "collection.add(\n",
    "    ids=[\"project_tech_01\"],\n",
    "    documents=[project_context],\n",
    "    metadatas=[{\"category\": \"Project Details\", \"source\": \"Architecture Docs\"}]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "c555664d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create IDs\n",
    "ids =[f\"chunk-{i}\" for i in range(len(texts))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "44d6a929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added chunks: 120\n"
     ]
    }
   ],
   "source": [
    "# Add documents to ChromaDB\n",
    "collection.add(\n",
    "    ids=ids,\n",
    "    documents=texts,\n",
    "    metadatas=metadatas,\n",
    "    embeddings=embeddings.tolist()\n",
    ")\n",
    "\n",
    "print(\"Added chunks:\", len(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "4cb17259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- RESULT 1 ---\n",
      "{'category': 'Resume', 'chunk_index': 0, 'source': 'Kevin_Johnson_res.pdf'}\n",
      "Kevin Johnson Austin, Texas ‚Ä¢ 832-514-9765 ‚Ä¢ kevin.johnson.data@gmail.com ‚Ä¢ LinkedIn | GitHub Education B.S. ...\n",
      "\n",
      "--- RESULT 2 ---\n",
      "{'category': 'Resume', 'source': 'Kevin_Johnson_res.pdf', 'chunk_index': 2}\n",
      "Science graduate (Summa Cum Laude) with hands-on experience building machine learning and applied AI systems, including Retrieval-Augmented Generation (RAG) and computer vision models. ...\n",
      "\n",
      "--- RESULT 3 ---\n",
      "{'category': 'Resume', 'chunk_index': 1, 'source': 'Kevin_Johnson_res.pdf'}\n",
      "Data Science | Arizona State University | 2023 ‚Äì 2025‚Äã - Graduated Summa Cum Laude (GPA 3.89)‚Äã - Completed projects in ML, classification, clustering, and CNNs B.B.A. in Logistics & Supply Chain Management | University of Houston | 2009 ‚Äì 2014‚Äã - NCAA Division I Decathlete, Team Captain Profile Data ...\n"
     ]
    }
   ],
   "source": [
    "query = \"education university degree college Kevin\"\n",
    "\n",
    "q_emb = embed_model.encode(\n",
    "    [query],\n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True\n",
    ")[0]\n",
    "\n",
    "results = collection.query(\n",
    "    query_embeddings=[q_emb],\n",
    "    n_results=3,\n",
    ")\n",
    "\n",
    "docs = results[\"documents\"][0]\n",
    "metas = results[\"metadatas\"][0]\n",
    "\n",
    "for i, (doc, meta) in enumerate(zip(docs, metas), start=1):\n",
    "    print(f\"\\n--- RESULT {i} ---\")\n",
    "    print(meta)\n",
    "    print(doc[:500], \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "41269c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ OpenAI Client Ready\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "ai_client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "if os.getenv(\"OPENAI_API_KEY\"):\n",
    "    print(\"‚úÖ OpenAI Client Ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a38760",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "35d39582",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(query,docs):\n",
    "    context = \"\\n\\n---\\n\\n\".join(docs)\n",
    "\n",
    "    return f\"\"\"\n",
    "You are Kevin's AI Career Assistant. Your job is to help recruiters understand Kevin's \n",
    "background by answering questions using ONLY the resume context provided below.\n",
    "\n",
    "Guidelines:\n",
    " - Be professional, enthusiastic, and concise.\n",
    " - If the answer isn't in the context, say: \"I'm sorry, Kevin's resume doesn't \n",
    "   specifically mention that, but you can reach out to him directly for more details.\"\n",
    " - Use bullet points for lists (like skills or project tasks).\n",
    " - Always frame answers to highlight Kevin's technical growth and problem-solving.\n",
    "\n",
    "Context from Kevin's Resume & Portfolio:\n",
    "{context}\n",
    "\n",
    "Recruiter Question:\n",
    "{query}\n",
    "\n",
    "Professional Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "7d42e80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(query, n_results=4):\n",
    "\n",
    "    # Easter Egg\n",
    "\n",
    "    if \"go coogs\" in query.lower() or \"decathlon\" in query.lower():\n",
    "        return (\n",
    "            \"üêæ **WHOOSE HOUSE?! COOGS HOUSE!** üêæ\\n\\n\"\n",
    "            \"You found the secret athlete mode! Kevin isn't just a Data Scientist; \\n\"\n",
    "            \"he's a D1 Decathlete who spent his college years hurdles-jumping \\nand \"\n",
    "            \"discus-throwing. If you're looking for a teammate with the stamina \\nof a \"\n",
    "            \"1500m runner and the explosive power of a shot-putter, you're looking at him!\",\n",
    "            [\"Internal Easter Egg\"], None, None\n",
    "        )\n",
    "\n",
    "    #Normalize query\n",
    "    query_clean = query.lower().replace(\"kevins\", \"kevin's\")\n",
    "    \n",
    "    # 1. Embed the query\n",
    "    q_emb = embed_model.encode(\n",
    "        [query_clean],\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=True,\n",
    "    )[0]\n",
    "    \n",
    "    # 2. Retrieve relevant chunks from Chroma\n",
    "    results = collection.query(\n",
    "        query_embeddings=[q_emb],\n",
    "        n_results=n_results\n",
    "    )\n",
    "\n",
    "    docs = results[\"documents\"][0]\n",
    "    metas = results[\"metadatas\"][0]\n",
    "    distances = results[\"distances\"][0]\n",
    "\n",
    "    # 3. Build prompt\n",
    "    prompt = build_prompt(query, docs)\n",
    "\n",
    "    # 4. LLM call\n",
    "    response = ai_client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are Kevin's professional Career Assistant. \"\n",
    "                                          \"Use the provided context to answer questions about his experience.\"\n",
    "                                          \"When asked about education, always look for BOTH of his undergraduate degrees.\"\n",
    "                                          \"He has a dual-degree background; ensure you mention both ASU and the University of Houston.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        max_tokens=500,\n",
    "        temperature=0.3,\n",
    "    )\n",
    "    \n",
    "    answer = response.choices[0].message.content\n",
    "\n",
    "    # 5. Build sources list\n",
    "    unique_sources = []\n",
    "    for meta in metas:\n",
    "        source_label = meta.get(\"category\", \"General\")\n",
    "        unique_sources.append(source_label)\n",
    "    \n",
    "    unique_sources = sorted(list(set(unique_sources)))\n",
    "\n",
    "    \n",
    "\n",
    "    return answer, unique_sources, docs, distances\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "0a810405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: Kevin possesses a diverse skill set that highlights his technical growth and problem-solving abilities in the field of Data Science. His key skills include:\n",
      "\n",
      "- **Data Analysis & Visualization:** Proficient in extracting insights from complex datasets and presenting them in a clear, actionable format.\n",
      "- **Machine Learning:** Experienced in developing and implementing machine learning models, including custom convolutional neural networks (CNNs).\n",
      "- **Programming Languages:** Proficient in Python, R, and SQL, enabling him to tackle various data-related challenges.\n",
      "- **Statistical Analysis:** Strong foundation in statistical methods, enhancing his ability to interpret data effectively.\n",
      "- **Project Management:** Demonstrated ability to manage projects from conception to execution, ensuring timely delivery of high-quality results.\n",
      "\n",
      "Kevin's combination of technical expertise and competitive spirit makes him a valuable asset in any data-driven environment.\n",
      "Sources: ['About Me', 'GitHub', 'Resume']\n"
     ]
    }
   ],
   "source": [
    "ans, src, d, dist = answer_question(\"What about Kevins skills?\")\n",
    "print(f\"AI: {ans}\")\n",
    "print(f\"Sources: {src}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "6ca4924d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- CHUNKS SENT TO AI ---\n",
      "Chunk 1:\n",
      "Kevin Johnson Austin, Texas ‚Ä¢ 832-514-9765 ‚Ä¢ kevin.johnson.data@gmail.com ‚Ä¢ LinkedIn | GitHub Education B.S.\n",
      "\n",
      "Chunk 2:\n",
      "Science graduate (Summa Cum Laude) with hands-on experience building machine learning and applied AI systems, including Retrieval-Augmented Generation (RAG) and computer vision models.\n",
      "\n",
      "Chunk 3:\n",
      "Continuous Learning: While the transfer learning models plateaued early, the custom CNN's accuracy curve was still climbing at the end of the training run, suggesting even higher potential with more\n",
      "\n",
      "Chunk 4:\n",
      "Professional Experience Machine Learning Engineer Intern | Green Rush Packaging | Fall 2025 - Built a Retrieval-Augmented Generation (RAG) model to answer state specific regulatory compliance questions from sales reps saving sales manager roughly 5 hours per week - Implemented document ingestion,\n",
      "\n",
      "Chunk 5:\n",
      "using classification models and behavioral analysis.\n",
      "\n",
      "Chunk 6:\n",
      "üìä The Dataset The project utilizes a comprehensive dataset (derived from the DAT 402 curriculum) containing customer demographic and behavioral features: Customer Profile: Age, occupation, education,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ans, src, docs, dist = answer_question(\"What is Kevin's education?\")\n",
    "\n",
    "print(\"--- CHUNKS SENT TO AI ---\")\n",
    "for i, d in enumerate(docs):\n",
    "    print(f\"Chunk {i+1}:\\n{d}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
